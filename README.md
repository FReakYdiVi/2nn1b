# Conditional Varitional Autoencoder for Deep generative models of Subsurface model

This is pytorch implementation of:
* Conditional Varitional Autoencoders(CVAE) using Convulution network 

## What is a CVAE?

A **CVAE** is a form of variational autoencoder that is
conditioned on an observation, where in our case the observation is a function y.

The autoencoders from which variational autoencoders
are derived are typically used for problems involving
image reconstruction and/or dimensionality reduction.

An Autoencoders is composed of two neural netwroks **Encoder** and **Decoder** where an Encoders takes an input prededined by the user and convert it into a low dimensionality space known as **latent space** and This latent space is passed through decoder to convert it to the original input size.

Additionally,
the decoder simultaneously learns to decode the latent
space representation and reconstruct that data back to
its original input. 

In **Varitional Autoencoders** this latent space is interpreted as a set of parameters governing statistical distributions In proceeding to the decoder
network, samples from the latent space (z) are randomly
drawn from these distributions and fed into the decoder,
therefore adding an element of variation into the process.
So in this way Varitional Autoencoders are used for generating samples of input which are similar to the given input(x) and **Conditional Autoencoder** is one step up in which we also add a condition that is to be fulfilled and this condition is passes through both encoder and decoder



# Referrals 
for data_1 please refer to [Starter_Notebook](https://github.com/FReakYdiVi/2nn1b/blob/main/Starter%20Notebook.ipynb) 

for data_2 please refer to [Starter Notebook1](https://github.com/FReakYdiVi/2nn1b/blob/main/Starter%20Notebook1.ipynb)


## Model Structure


| Neural Network | Number of CNN Layers | Number of Linear Layers | Activation Function       |
|-----------|----------------------|-------------------------|---------------------------|
| Encoder   | 2                    | 1                       |  ReLU     |
| Decoder   | 2                    | 1                       | LeakyReLU (0.01 |


### Network Structure

Refer to [CVAE_example.py](https://github.com/FReakYdiVi/2nn1b/blob/main/CVAE_example.py) for model code and structure names as CVAE 


## Model Structure related to research paper 

<img width="394" alt="Screenshot 2024-02-09 at 2 45 54 PM" src="https://github.com/FReakYdiVi/2nn1b/assets/129744247/33204703-dd77-45f3-a7bd-f4c7df1176b8">

### Explanation
The encoder (denoted by qφ(z|x) with
φ the abbreviation of biases and weights of the encoder’s
neural network) compress the input data x into the latent
layer z of smaller dimensions than the ones of x, and then
the decoder (denoted by pθ(˜x|z) with θ the abbreviation
of biases and weights of the decoder’s neural network)
un-compress the latent layer back to the final result ˜x of
the same dimension as x. and Due to the additional encoder, we now have two latent
vectors z1 and z2 as shown in [img](<img width="394" alt="Screenshot 2024-02-09 at 2 45 54 PM" src="https://github.com/FReakYdiVi/2nn1b/assets/129744247/33204703-dd77-45f3-a7bd-f4c7df1176b8">
). We can then introduce the latent loss to measure their difference. For for CVAE it is the KL divergence between the Gaussian distributions generated by the two encoders. On the
other hand, the reconstruction loss is the MSE between the x and reconstructed x and cross entropy loss for y and reconstructed y


## Model Structure related to research paper 

| Component   | Number of CNN Layers | Number of Linear Layers | Activation Functions Used |
|-------------|----------------------|-------------------------|---------------------------|
| Encoder1    | 2                    | 2                      | ReLu          |
| Encoder2    | 0                    | 2                       | -          |
| Decoder     | 2                    | 1                       | LeakyRelu          |

### Network Structure

Refer to [CVAE_example.py](https://github.com/FReakYdiVi/2nn1b/blob/main/CVAE_example.py) for model code and structure names as CVAE_re

If you want to know more about the structure , just read this [reaserch paper](https://arxiv.org/abs/2101.06685
)

## Usage Of Libararies
### pre requistes
1. pytorch , numpy ,matplot , pandas
### Built In Library
1. CVAE_example, CVAE_functions 


## Results

For data 1:

* reconstruction_error0: 0.9570026993751526

* reconstruction_error1: 0.9536468386650085

* linearity_score0: 0.3876624462311889

* linearity_score1: 0.33577910396708593

For data 2:

* reconstruction_error0: 1.00575852394104

* reconstruction_error1: 0.7738876342773438

* linearity_score0: 0.9033710532064656

* linearity_score1: 0.7145611655175658

## Blockers

This was tough and new kind of problem for me as it was related to geneartive ai and through this journey i got to many insights towards how autoencoders work or particulary how Conditional variational autoencoders work at generating samples and there were many hurdles towards how to make this project a better performer so I am gonna share a few and how i solved it -

1. the first problem was with **tuning of hyperparameters**
and I was using **optuna** for this but i was not getting good results with the optuna so then i thought to drop the idea of hyperparameter tuning and move towards upgrading the model structure of given model and thought i would be more relelvant.

2. I just modify the given model by adding leaky relu and it greatly reduces the overall loss of the model and secondly i also added condition to the encoder part as it was firstly not passing through encoder.

3. I also modify the given structure by adding one more convulution layer and you can see this code in highlighted from in [CVAE_example.py](https://github.com/FReakYdiVi/2nn1b/blob/main/CVAE_example.py)

4. I got know about the research paper then i tried to build the similar structure just to make **two encoders and decoder** so firstly it was hard to make such structure and also updating the functions according to the given structure  and i also changed the y_loss function to cross entropy which was more better than MSE but finally i made the similar model but it took too much time as it was very tough to make
